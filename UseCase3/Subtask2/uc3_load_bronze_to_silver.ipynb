{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, explode\r\n",
        "from pyspark.sql.functions import collect_list\r\n",
        "from pyspark.sql.functions import col, concat_ws"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkPool",
              "statement_id": 30,
              "statement_ids": [
                30
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-02-11T10:03:38.8207095Z",
              "session_start_time": null,
              "execution_start_time": "2025-02-11T10:03:38.9504962Z",
              "execution_finish_time": "2025-02-11T10:03:39.0966155Z",
              "parent_msg_id": "ea4bda57-2c52-4a5e-9d5f-7c9db0e3cb60"
            },
            "text/plain": "StatementMeta(sparkPool, 0, 30, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#notebook parameters. Leave them empty\n",
        "in_file_genres = \"bronze/tmdb/tmdb_genres_20250210.json\"\n",
        "in_file_trending = \"bronze/tmdb/tmdb_trending_item_20250210.json\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkPool",
              "statement_id": 31,
              "statement_ids": [
                31
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-02-11T10:03:38.8898061Z",
              "session_start_time": null,
              "execution_start_time": "2025-02-11T10:03:39.2366547Z",
              "execution_finish_time": "2025-02-11T10:03:39.3944485Z",
              "parent_msg_id": "cc931e3e-45d2-46b0-9115-c491cd876697"
            },
            "text/plain": "StatementMeta(sparkPool, 0, 31, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false,
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: uncomment the row below and assign your data lake storage name to the variable\r\n",
        "storage_account = \"stdimentoringdatalakeed\"\r\n",
        "\r\n",
        "container = \"data\"\r\n",
        "\r\n",
        "date_postfix = in_file_genres[in_file_genres.find('.json') - 8 : in_file_genres.find('.json')]\r\n",
        "\r\n",
        "file_genres = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{in_file_genres}\"\r\n",
        "\r\n",
        "#TODO: uncomment the row below and assign movies trending file path to the ariable\r\n",
        "file_trending = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{in_file_trending}\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkPool",
              "statement_id": 32,
              "statement_ids": [
                32
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-02-11T10:03:38.9767074Z",
              "session_start_time": null,
              "execution_start_time": "2025-02-11T10:03:39.5476402Z",
              "execution_finish_time": "2025-02-11T10:03:39.7139237Z",
              "parent_msg_id": "e2f081ca-9c17-49dc-a08a-5dde1213f551"
            },
            "text/plain": "StatementMeta(sparkPool, 0, 32, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load list of genres to a dataframe"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "#TODO: uncomment the row below and complete missing parts with your code to create spark dataframe from file_genres source using spark.read.load method\r\n",
        "df_genres = spark.read.json(file_genres)\r\n",
        "\r\n",
        "#explode genres array\r\n",
        "df_genres = df_genres.select(explode('genres').alias('genres')).select('genres.id','genres.name')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkPool",
              "statement_id": 33,
              "statement_ids": [
                33
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-02-11T10:03:39.1137093Z",
              "session_start_time": null,
              "execution_start_time": "2025-02-11T10:03:39.8650164Z",
              "execution_finish_time": "2025-02-11T10:03:42.6981449Z",
              "parent_msg_id": "dc9fab29-fb6e-4bb9-8e10-96b30b2ded8e"
            },
            "text/plain": "StatementMeta(sparkPool, 0, 33, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load list of trending movies to a dataframe"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "#TODO: uncomment the row below and complete missing parts with your code to create spark dataframe from file_trending source using spark.read.load method\r\n",
        "df_movies = spark.read.json(file_trending)\r\n",
        "\r\n",
        "#explode \"results\" property array\r\n",
        "df_movies = df_movies.select(explode('results').alias('movies')).select('movies.id', 'movies.title', 'movies.popularity', 'movies.vote_average', 'movies.genre_ids')\r\n",
        "\r\n",
        "#explode \"genres_ids\" property array\r\n",
        "df_movies = df_movies.select(df_movies.id.alias('movie_id'), 'title', 'popularity', 'vote_average', explode('genre_ids').alias('genre_id'))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkPool",
              "statement_id": 34,
              "statement_ids": [
                34
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-02-11T10:03:39.4282914Z",
              "session_start_time": null,
              "execution_start_time": "2025-02-11T10:03:42.845898Z",
              "execution_finish_time": "2025-02-11T10:03:43.9323584Z",
              "parent_msg_id": "f3011076-ee4d-44bf-9dbe-0f598d3898a1"
            },
            "text/plain": "StatementMeta(sparkPool, 0, 34, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#joining movies with genres dictionary to get genre name\r\n",
        "inner_join = df_movies.join(df_genres,\r\n",
        "     (df_movies.genre_id == df_genres.id), how = 'inner').select(df_movies.movie_id, df_movies.title, df_movies.popularity, df_movies.vote_average, df_genres.name.alias('genre_name'))\r\n",
        "\r\n",
        "#collect movie genres to a list of strings\r\n",
        "grouped_genres = inner_join.groupBy('movie_id', 'title', 'popularity', 'vote_average').agg(collect_list('genre_name').alias('genres'))\r\n",
        "\r\n",
        "#get concatenated string value from list of genres\r\n",
        "result = grouped_genres.withColumn(\"genres\",\r\n",
        "   concat_ws(\",\",col(\"genres\")))     "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkPool",
              "statement_id": 35,
              "statement_ids": [
                35
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-02-11T10:03:39.7502328Z",
              "session_start_time": null,
              "execution_start_time": "2025-02-11T10:03:44.0745673Z",
              "execution_finish_time": "2025-02-11T10:03:44.2308819Z",
              "parent_msg_id": "f937a2ee-07af-4715-bbef-9a2e01f26677"
            },
            "text/plain": "StatementMeta(sparkPool, 0, 35, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write resulted dataframe to a parquet file in silver layer\r\n",
        "write_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/tmdb/tmdb_trending_{date_postfix}\"\r\n",
        "\r\n",
        "#TODO: uncomment the row below and complete missing parts with your code to save \"result\" dataframe to data lake in parquet format using overwrite moe\r\n",
        "result.write.parquet(write_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkPool",
              "statement_id": 36,
              "statement_ids": [
                36
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-02-11T10:03:40.0659464Z",
              "session_start_time": null,
              "execution_start_time": "2025-02-11T10:03:44.3675337Z",
              "execution_finish_time": "2025-02-11T10:03:55.5145298Z",
              "parent_msg_id": "7e2c166f-d91b-4506-809d-32d9365887cc"
            },
            "text/plain": "StatementMeta(sparkPool, 0, 36, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}