{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bde1093e-b419-4d59-b4fd-579d00e45a08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7fd5f0b0cc10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Locations\n",
    "bronzeFilePath = \"/mnt/data/bronze/events\"\n",
    "silverFilePath = \"/mnt/data/silver/events\"\n",
    "quarantineFilePath = \"/mnt/data/quarantine/events\"\n",
    "checkpointPath = \"/mnt/data/checkpoints/events\"\n",
    "checkpointPathQuarantine = \"/mnt/data/checkpoints/events_quarantine\"\n",
    "silverMoviesPath = \"/mnt/data/silver/imdb/titles\"\n",
    " \n",
    "#TODO: Data Quality Checks\n",
    "\n",
    "# Please fill-in gaps \n",
    "# Here is a dictionary \"rules\" created where all rules will be stored as key-value pairs where key is a rule name and value is an expression to be run in dataframe, \n",
    "# e.g. {\"valid_event_timestamp\":\"(event_timestamp IS NOT NULL)\", ...}\n",
    "# Then in \"quarantine_rules\" variable stored a string with expression generated with all checks \n",
    "# like \"NOT((event_timestamp IS NOT NULL) AND (other rule) AND (other rule))\"\n",
    "# Your goal is to fill in rules for now\n",
    "\n",
    "# • Rule#1: event_type in “START” or “FINISH” state.\n",
    "# • Rule#2: event_uid is not empty.\n",
    "# • Rule#3: event_id is not empty.\n",
    "# • Rule#4: user_subscription_device_id is not empty.\n",
    "# • Rule#5: movie_id is not empty.\n",
    "# • Rule#6: movie_id value exists in \"silver/imdb/titles/\" entity (title_id column).\n",
    "\n",
    "rules = {}\n",
    "rules[\"valid_event_type\"] = \"(event_type='START' OR event_type='FINISH')\"\n",
    "rules[\"valid_event_uid\"] = \"(event_uid IS NOT NULL)\"\n",
    "rules[\"valid_event_id\"] = \"(event_id IS NOT NULL)\"\n",
    "rules[\"valid_user_subscription_device_id\"] = \"(user_subscription_device_id IS NOT NULL)\"\n",
    "rules[\"valid_movie_id\"] = \"(movie_id IS NOT NULL)\" \n",
    "quarantine_rules = \"NOT({0})\".format(\" AND \".join(rules.values()))\n",
    "\n",
    "#TODO: Read movies dataset from silver for dq check. \n",
    "# This dataframe will be used for Rule#6\n",
    "# <read \"title_id\" column dataset from 'silver/imdb/titles' dataset>\n",
    "moviesDF = spark.read.parquet(silverMoviesPath).select(\"title_id\") \n",
    "\n",
    "#TODO: Read bronze layer using Auto Loader\n",
    "bronzeDF = (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"cloudFiles.schemaLocation\", checkpointPath)\n",
    "    .option(\"cloudFiles.checkpointLocation\", checkpointPath)\n",
    "    .option(\"cloudFiles.validateOptions\", \"false\")\n",
    "    .load(bronzeFilePath)\n",
    "    .withColumn(\"is_quarantined\", expr(quarantine_rules))\n",
    ")\n",
    "\n",
    "bronzeDF = bronzeDF.join(moviesDF, bronzeDF.movie_id == moviesDF.title_id, \"left\")\n",
    "\n",
    "#Finalize dataset for silver layer \n",
    "silverDF = (\n",
    "        bronzeDF.filter((bronzeDF.is_quarantined == False) & (bronzeDF.title_id.isNotNull()))\n",
    "        .drop(\"is_quarantined\", \"title_id\", \"_rescued_data\")\n",
    "        .withColumn(\"year\", date_format(col(\"event_timestamp\"), \"yyyy\"))\n",
    "        .withColumn(\"month\", date_format(col(\"event_timestamp\"), \"MM\"))\n",
    "        .withColumn(\"day\", date_format(col(\"event_timestamp\"), \"dd\"))\n",
    ")\n",
    "\n",
    "#TODO: Finalize dataset for quarantine\n",
    "# Rows which do not meet the data quality rules should be written to the quarantine folder\n",
    "# <fill in code to gerenerate dataframe with invalid data>\n",
    "silverDF_quarantine = (\n",
    "        bronzeDF.filter(bronzeDF.is_quarantined == True)\n",
    "        .withColumn(\"year\", date_format(col(\"event_timestamp\"), \"yyyy\"))\n",
    "        .withColumn(\"month\", date_format(col(\"event_timestamp\"), \"MM\"))\n",
    "        .withColumn(\"day\", date_format(col(\"event_timestamp\"), \"dd\"))\n",
    ")\n",
    "\n",
    "#TODO: Write to silver layer\n",
    "#fill in code to write data from stream into silverFilePath in parquet format\n",
    "#use checkpointPath for checkpoint location\n",
    "#partition data by year, month, day\n",
    "(\n",
    "        silverDF.writeStream\n",
    "        .option(\"checkpointLocation\", checkpointPath)\n",
    "        .partitionBy(\"year\", \"month\", \"day\")\n",
    "        .format(\"parquet\")\n",
    "        .outputMode(\"append\")\n",
    "        .start(silverFilePath)\n",
    ")\n",
    "\n",
    "#TODO: Write to quarantine\n",
    "#fill in code to write data from stream into quarantineFilePath folder into subdirectory named by loading datetime (format should be 2023-03-01-050000)in parquet format\n",
    "loading_datetime = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n",
    "quarantineFilePathWithDatetime = f\"{quarantineFilePath}/{loading_datetime}\"\n",
    "\n",
    "(\n",
    "        silverDF_quarantine.writeStream\n",
    "        .option(\"checkpointLocation\", checkpointPathQuarantine)\n",
    "        .format(\"parquet\")\n",
    "        .outputMode(\"append\")\n",
    "        .start(quarantineFilePathWithDatetime)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4259232939495986,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "uc4_load_bronze_to_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}